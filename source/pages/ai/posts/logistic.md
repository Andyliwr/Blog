### logistic 回归将会用到的符号讲解

logistic 回归是一个用于二分分类的算法，输入一个 x，输出的值 y 是 0 或者 1

举个简单的例子，现在有一张图片，算法需要判断图片中是否出现了猫，如果出现了记结果为 1，否则结果为 0
计算机表示图片的方式是将每个像素点描述成一个 RGB 的三元组，这样一张 64x64 像素的图片就需要 3 个 64\*64 的矩阵来表示，矩阵中的每一项都代表着当前图片位置的 RGB 的值。

![图片的表示](https://fs.andylistudio.com/1533654168188.png)

然后我们需要将三个矩阵转化成一个特征向量，特征向量是一个 1x(3*64*64)的矩阵(特征向量的维度就是 3*64*64=12288)，也就是 R 矩阵从左往右依次竖着排列，接下来是 G 矩阵，最后是 B 矩阵。

我们将特征向量作为 x 值输入，得到的 0 或者 1 作为 y 值，x 和 y 组成的二维组(x, y)就是一个训练样本，n 个训练样本就组成了一个训练集---{(x1, y1), (x2, y2) ... (xm, ym)}。

在训练神经网络模型的时候，x 的值可能是多维度的，每个 x 的值作为矩阵的一列，最后的输入集就是 X = [x1, x2, ... xm], 假设每个 x 的维度是 nx，那么输入集的维度就是 m\*nx。Y 作为输出集，可以表示为 Y = [y1, y2, ... yn]

![符号说明](https://fs.andylistudio.com/1533654172009.png)

[课件下载](https://fs.andylistudio.com/1533739787344.pdf)

### 正式讲解 logistic 回归

之前提到的例子，我们需要判断一张图片中是否包含猫，如果包含输出 1，不包含输出 0，转化下其实就是求解这张图片中出现猫的概率，肯定出现就是 1，肯定不出现就是 0，所以 x 是一个特征向量，y 则是一个 0 到 1 的实数。

先来试下线性回归，需要两个参数--斜率 w，以及偏移值 b，y=wx+b，这是最简单的回归，然而却存在很大的问题，当 x 足够到的时候 y 也会变成无穷大或者无穷小，无法将 y 的值限制在 0 和 1 之间，因此我们需要一个额外的函数 f，f(z) = 1 / (1+e^-z)，这样到 z 足够到的时候 e^-z 就会趋近于 0，f(z)就会趋近于 1，当 z 无穷小的时候，e^-z 就会趋近于无穷大，f(z)就会趋近于 0

真正的 logisti 回归 w 和 x 并非一个常量，b 可能是一个偏移矩阵，就像这样 b=[d1, d2, d3, ... dnx]，当然它是竖着的。w 也可能是一个矩阵。

![参数说明](https://fs.andylistudio.com/1533740910262.png)

[课件下载](https://fs.andylistudio.com/1533744466749.pdf)

给定 m 个样本，我们希望能求算出 w 以及 b，使得得出的结果尽可能的接近样本结果值，因此我们需要一个误差函数来衡量 w 和 b 的合理程度。
对于每个样本值 xi, 假设结果是 yi，根据 logistic 算法得到的结果是 zi, 那么预测值和实际结果值之间的距离可以表示为 (zi - yi)^2 / 2，这样我们就可以定义误差函数（又叫损失函数，用来衡量预测结果和实际结果的接近程度）了

#### 损失函数

通过平方差来定义损失函数，可以会得到多个误差相近的模型，也就是可能会有多个最优解。我们可以使用另一个损失函数，f(yi, zi) = -(yi \* log(zi) + (1-yi) \* log(1-zi))，当 yi = 1 的时候，f = - (1 - log(zi))，为了使得损失值 f 越小，就得使得 log(zi) 越大，也就是 zi 越大，但是 zi 再大也不会超过 1，zi 越接近 1，损失值 f 就越趋近 0；当 yi = 0 的时候，f = - log(1-zi)，为了使得损失值 f 越小，就得使得 log(1-zi)越大，这样 zi 就必须足够的小，zi 越趋近于 0，损失值 f 就越趋近于 0。看，这完全符合现实。

损失函数并非只有这一种，有很多都能实现上述的效果。

#### 成本函数

损失函数是体现单个样本的误差程度，成本函数则是衡量当前模型在全体训练样本上的误差程度（就是求平均值）

损失函数 j = (f(y1, z1) + f (y2, z2) + ... + f(ym,zm)) / m

我们的最终目标就是要找到合适的 w 和 b，使得成本函数 j 的值越小越好

![损失函数和成本函数](https://fs.andylistudio.com/1533746571391.png)

#### 结尾

logistic 回归在真正的神经网络学习里可能只是一个非常小的神经网络，多个小的神经网络组合起来才能用于去解决更加复杂的问题。
